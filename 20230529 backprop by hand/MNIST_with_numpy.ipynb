{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dungwoong/NN/blob/main/MNIST_with_numpy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Numpy for calculations"
      ],
      "metadata": {
        "id": "-OCldum_19q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.random import randn"
      ],
      "metadata": {
        "id": "86NV72DcxGaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module classes\n",
        "Node classes are more like modules. They all implement a forward function that returns outputs of a forward pass and a backwards function that computes gradients\n",
        "\n",
        "I decided to store x, ans inside the node for use when computing gradients. ```input_grad``` is the gradient with respect to the inputs to each module.\n",
        "\n",
        "Gradients with respect to module parameters are returned by the backwards() function.\n",
        "\n",
        "Gradients are not accumulated, they are replaced with each backwards() call, thus zero_grad is not required.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "**Difference to existing libraries:**\n",
        "\n",
        "Existing libraries automatically build a computation graph.\n",
        "For my implementation, we must manually build a computation graph and perform topological sorting in order to backpropagate correctly. This is undesirable, but is suitable for my purpose of demonstrating backprop.\n",
        "\n",
        "<br>\n",
        "\n",
        "In addition, the computation graph is modular, and is thus very simple. Most libraries follow tensor operations to build a computation graph, and allow us to run a backwards() function on the result of any computation to compute gradients. I opted for simpler functionality for my problem."
      ],
      "metadata": {
        "id": "_KNiAtkw2KsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the backprop classes will only support having 1 child, for a simple linear network\n",
        "# normally you have multiple children, and aggregate the gradient signals from them.\n",
        "\n",
        "class Node:\n",
        "  \"\"\"\n",
        "  Base class for a single node in the computation graph.\n",
        "\n",
        "  Acts more like a module. Nodes only have 1 child, so the computation graph\n",
        "  isn't really a graph.\n",
        "\n",
        "  Nodes have forwards and backwards methods implemented.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.ans, self.x = None, None\n",
        "    self.input_grad = 0\n",
        "    self.parameters = []\n",
        "    self.child = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backwards(self, g):\n",
        "    # must save gradients WRT inputs in self.input_grad, return a list with same length as self.parameters with gradients for each respective parameter\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self.forward(*args, **kwargs)"
      ],
      "metadata": {
        "id": "4kOqeHIfJ5V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear layer\n",
        "\n",
        "Performs an affine transformation."
      ],
      "metadata": {
        "id": "9Ett4iPZ41K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearLayer(Node):\n",
        "  \"\"\"\n",
        "  Linear layer, goes from input_size to output_size.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size, output_size, bias=False, init_func=randn):\n",
        "    super().__init__()\n",
        "    self.w = init_func(output_size, input_size)\n",
        "    self.b = init_func(output_size, 1) if bias else None\n",
        "    self.parameters = [self.w, self.b] if bias else [self.w]\n",
        "\n",
        "  def kaiming_init(self):\n",
        "    \"\"\"\n",
        "    Kaiming initialization\n",
        "    \"\"\"\n",
        "    factor = np.sqrt(2 / self.w.shape[1])\n",
        "    self.w = randn(*self.w.shape) * factor\n",
        "    # self.b = randn(*self.b.shape) * factor\n",
        "    self.b = np.zeros(self.b.shape) if self.b is not None else None\n",
        "  \n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    Calculates Wx + b. Thus, x is input_size x batch_size\n",
        "    \"\"\"\n",
        "    self.x = x\n",
        "    self.ans = self.w @ x\n",
        "    if self.b is not None:\n",
        "      self.ans += self.b\n",
        "    return self.ans\n",
        "\n",
        "  def backwards(self, g):\n",
        "    self.input_grad = self.w.T @ g\n",
        "\n",
        "    b_grad = [g @ np.ones((g.shape[1], 1))] if self.b is not None else []\n",
        "    return [g @ self.x.T] + b_grad"
      ],
      "metadata": {
        "id": "ZbcNEoFh2J10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other modules\n",
        "\n",
        "These are other modules that will come in handy sometime or another. Implemented the same way as the other modules.\n",
        "\n",
        "We have:\n",
        "\n",
        " - ReLU(rectified linear unit): $max(0, x)$\n",
        " - MSE(mean squared error): $\\frac{1}{2}||y - t||_2^2$"
      ],
      "metadata": {
        "id": "tpa1DfZ75BO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(Node):\n",
        "  \"\"\"\n",
        "  Rectified linear unit\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.x = x\n",
        "    self.ans = np.maximum(0, x)\n",
        "    return self.ans\n",
        "\n",
        "  def backwards(self, g):\n",
        "    jacobian = np.where(self.x > 0, 1, 0)\n",
        "    self.input_grad = jacobian * self.ans\n",
        "\n",
        "\n",
        "class MSE(Node):\n",
        "  \"\"\"\n",
        "  Mean squared error\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.t = None\n",
        "  \n",
        "  def forward(self, x, t):\n",
        "    self.x = x\n",
        "    self.t = t\n",
        "    self.ans = 0.5 * np.sum((self.x - self.t)**2)\n",
        "    return self.ans\n",
        "\n",
        "  def backwards(self, g):\n",
        "    self.input_grad = self.x - self.t\n"
      ],
      "metadata": {
        "id": "4kPNW7tz4z1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagate function\n",
        "\n",
        "This function will pass back gradient signals along a list of topologically sorted nodes to perform backprop."
      ],
      "metadata": {
        "id": "-GXxDnCX5toL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_propagate(top_sorted_nodes):\n",
        "  \"\"\"\n",
        "  Performs backprop.\n",
        "\n",
        "  Stores output in a dict in the format {node: <grads>}\n",
        "  where <grads> is a list of grads the same length as the number of parameters in the node.\n",
        "  \"\"\"\n",
        "  grads = dict()\n",
        "  for node in top_sorted_nodes:\n",
        "    in_grad = node.child.input_grad if node.child is not None else 1\n",
        "    g = node.backwards(in_grad)\n",
        "    if g is not None:\n",
        "      grads[node] = g\n",
        "  return grads"
      ],
      "metadata": {
        "id": "nDtb4BI-_eKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizer and dataset\n",
        "\n",
        "I used a basic SGD optimizer."
      ],
      "metadata": {
        "id": "1IKAo0755md8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGD:\n",
        "  \"\"\"\n",
        "  SGD optimizer\n",
        "\n",
        "  x' = x - lr * dL/dx\n",
        "  \"\"\"\n",
        "  def __init__(self, lr):\n",
        "    self.lr = lr\n",
        "\n",
        "  def step(self, grads):\n",
        "    for node in grads:\n",
        "      for i in range(len(node.parameters)):\n",
        "        node.parameters[i] -= self.lr * grads[node][i]\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "  \"\"\"\n",
        "  Dataset class, can quickly generate a batched iterator.\n",
        "  \"\"\"\n",
        "  def __init__(self, inputs, targets):\n",
        "    self.inputs = inputs\n",
        "    self.targets = targets\n",
        "    self.dataset_size = inputs.shape[0]\n",
        "\n",
        "  def get_batches(self, batch_size):\n",
        "    num_batches = int(np.ceil(self.dataset_size / batch_size))\n",
        "    return num_batches\n",
        "\n",
        "  def batch_iterator(self, batch_size):\n",
        "    indices = np.arange(self.dataset_size)\n",
        "    np.random.shuffle(indices)  # Shuffle the indices\n",
        "    num_batches = int(np.ceil(self.dataset_size / batch_size))\n",
        "\n",
        "    for batch_num in range(num_batches):\n",
        "        start_index = batch_num * batch_size\n",
        "        end_index = min((batch_num + 1) * batch_size, self.dataset_size)\n",
        "        batch_indices = indices[start_index:end_index]\n",
        "        yield self.inputs[batch_indices], self.targets[batch_indices]"
      ],
      "metadata": {
        "id": "_79nJReVR6lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a simple linear regression model"
      ],
      "metadata": {
        "id": "C-xXqUOG6clQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config params\n",
        "np.random.seed(42)\n",
        "dataset_size = 2000\n",
        "batch_size = 64\n",
        "slope = np.array([3., 4., 2.]).reshape(3, 1)\n",
        "intercept = 5\n",
        "noise_level = 0.5\n",
        "nepochs = 10\n",
        "\n",
        "# Setting up the dataset\n",
        "x = randn(dataset_size, 3)\n",
        "y = (x @ slope) + intercept + (randn(dataset_size, 1) * noise_level)\n",
        "data = Dataset(x, y)\n",
        "\n",
        "# Setting up the modules and computation graph as linear --> MSE\n",
        "linear = LinearLayer(input_size=3, output_size=1, bias=True)\n",
        "mse = MSE()\n",
        "linear.child = mse\n",
        "nodes = [mse, linear]\n",
        "\n",
        "optimizer = SGD(lr=0.001)\n",
        "\n",
        "# Training\n",
        "print(f\"Targets are w={slope}, b={intercept}\")\n",
        "print(f\"{data.get_batches(batch_size)} batches in dataset of size {dataset_size}\")\n",
        "\n",
        "for epoch in range(nepochs):\n",
        "  for x, t in data.batch_iterator(batch_size):\n",
        "    # x is batch_size x 1, want to change to 1 x batch_size by transposing\n",
        "    y = linear(x.T)\n",
        "    loss = mse(y, t.T)\n",
        "\n",
        "    grads = back_propagate(nodes)\n",
        "    optimizer.step(grads)\n",
        "  print(f\"Epoch {epoch+1}: W={linear.w[0]}, b={linear.b[0]}, Loss={loss}\")\n",
        "\n",
        "# print(f\"Final: W={linear.w[0]}, b={linear.b[0]}, Loss={loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGARgnlxTQcJ",
        "outputId": "47168936-29aa-4ca6-c0ef-446ad2d7118e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Targets are w=[[3.]\n",
            " [4.]\n",
            " [2.]], b=5\n",
            "32 batches in dataset of size 2000\n",
            "Epoch 1: W=[2.51916934 3.34408372 1.73531408], b=[4.42896598], Loss=8.759483008415106\n",
            "Epoch 2: W=[2.91275733 3.91033482 1.97277158], b=[4.92334944], Loss=1.4365836037756925\n",
            "Epoch 3: W=[2.9763836  3.99070275 2.00380442], b=[4.97975813], Loss=1.713785224365796\n",
            "Epoch 4: W=[2.986183   3.99928556 1.99800597], b=[4.99938726], Loss=3.424956772394891\n",
            "Epoch 5: W=[2.98992147 3.99752928 1.99911202], b=[4.99348796], Loss=2.603850806885471\n",
            "Epoch 6: W=[2.97929336 3.99784    2.00269223], b=[5.0024308], Loss=2.3286014020096886\n",
            "Epoch 7: W=[2.98527337 4.01063784 2.01272288], b=[4.99269835], Loss=2.384166452896827\n",
            "Epoch 8: W=[2.98568039 4.01872176 2.00181709], b=[4.98184452], Loss=2.1887163308641036\n",
            "Epoch 9: W=[2.97291501 4.00818726 2.00308851], b=[4.99412349], Loss=1.8316835790255352\n",
            "Epoch 10: W=[2.98202304 3.99832889 1.99933981], b=[4.99501809], Loss=2.486623610024849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST\n",
        "\n",
        "todo figure out chatGPT softmax derivative\n",
        "\n",
        "https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1"
      ],
      "metadata": {
        "id": "jEv2do9pxLQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax(Node):\n",
        "  # by default, inputs should be input_size x batch_size, so we softmax along dim 0\n",
        "\n",
        "  def forward(self, x):\n",
        "    # TESTED\n",
        "    # x should be input_size x batch_size\n",
        "    self.x = x\n",
        "    x = np.exp(x)\n",
        "    sums = np.sum(x, axis=0, keepdims=True)\n",
        "    self.ans = x / sums\n",
        "    return self.ans\n",
        "\n",
        "  def backwards1(self, g):\n",
        "    # my backwards() implementation\n",
        "    out = []\n",
        "    # ans will be input_size x batch_size, we wanna end up with input_size x 1 vectors\n",
        "    for i in range(self.ans.shape[1]): # go thru obs\n",
        "      obs = np.expand_dims(self.ans[:, i], 1)\n",
        "      grad = np.expand_dims(g[:, i], 1)\n",
        "      m = -obs @ obs.T # input_size x input_size\n",
        "      np.fill_diagonal(m, obs * (1 - obs))\n",
        "      out.append(m.T @ grad) # input_size x 1\n",
        "    # output should be input_size x batch_size\n",
        "    self.input_grad = np.stack(out).reshape(self.ans.shape)\n",
        "\n",
        "  # more efficient implementation, by ChatGPT\n",
        "  # I don't even think this derivative is correct, it might just be proportional\n",
        "  # pretty sure this is the derivative for sigmoid...\n",
        "  def backwards2(self, g):\n",
        "    batch_size = self.x.shape[1]\n",
        "    dx = self.ans * (g - np.sum(self.ans * g, axis=0, keepdims=True))\n",
        "    self.input_grad = dx\n",
        "\n",
        "  def backwards(self, g):\n",
        "    self.backwards2(g)\n",
        "\n",
        "\n",
        "class NLLLoss(Node):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.t = None\n",
        "\n",
        "  def forward(self, x, t):\n",
        "    # x, t are num_classes x batch_size\n",
        "    self.x = x\n",
        "    self.t = one_hot_encode(t)\n",
        "    l = -np.log(x)\n",
        "    self.ans = np.trace(self.t.T @ l) # b x b\n",
        "    return self.ans\n",
        "  \n",
        "  def backwards(self, g=1):\n",
        "    self.input_grad = -np.reciprocal(self.x) * self.t\n",
        "\n",
        "\n",
        "def one_hot_encode(x, num_classes=10):\n",
        "  \"\"\"\n",
        "  One hot encodes x if x is a (size,) vector of class labels.\n",
        "  \"\"\"\n",
        "  if len(x.shape) != 1:\n",
        "    return x\n",
        "  y = np.zeros((num_classes, x.shape[0]))\n",
        "  y[x, np.arange(x.shape[0])] = 1\n",
        "  return y\n"
      ],
      "metadata": {
        "id": "bASOuhOIUTb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing if gradients work(DELETE LATER)\n",
        "\n",
        "softmax + NLL seems to work."
      ],
      "metadata": {
        "id": "u4yo_jD55oJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = NLLLoss()\n",
        "softmax = Softmax()\n",
        "softmax.child = loss\n",
        "\n",
        "targets = np.array([[0, 1], [1, 0], [0, 1]]).T\n",
        "inputs = np.array([[10, -10], [-10, 10], [10, -10]], dtype=float).T\n",
        "\n",
        "module = [loss, softmax]\n",
        "\n",
        "for i in range(500):\n",
        "  logits = softmax(inputs)\n",
        "  l = loss(logits, targets)\n",
        "  grads = back_propagate(module)\n",
        "  inputs -= softmax.input_grad\n",
        "  if i % 100 == 0:\n",
        "    print(f\"{i}: loss={l}\")\n",
        "    print(\"Logits:\", logits.T.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVsvpaEB7LfF",
        "outputId": "1a24136d-3948-4c49-aa87-6ee3bbc42444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: loss=60.000000006183456\n",
            "Logits: [[0.9999999979388463, 2.0611536181902033e-09], [2.0611536181902033e-09, 0.9999999979388463], [0.9999999979388463, 2.0611536181902033e-09]]\n",
            "100: loss=0.016601453438203247\n",
            "Logits: [[0.005518534447723954, 0.9944814655522761], [0.9944814655522761, 0.005518534447723954], [0.005518534447723954, 0.9944814655522761]]\n",
            "200: loss=0.007880089523236312\n",
            "Logits: [[0.0026232497589882965, 0.9973767502410117], [0.9973767502410117, 0.0026232497589882965], [0.0026232497589882965, 0.9973767502410117]]\n",
            "300: loss=0.005166124417087274\n",
            "Logits: [[0.0017205596096779036, 0.9982794403903221], [0.9982794403903221, 0.0017205596096779036], [0.0017205596096779036, 0.9982794403903221]]\n",
            "400: loss=0.0038426757489169313\n",
            "Logits: [[0.001280071924399478, 0.9987199280756006], [0.9987199280756006, 0.001280071924399478], [0.001280071924399478, 0.9987199280756006]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if the two backwards methods calculate the same gradient\n",
        "loss = NLLLoss()\n",
        "softmax = Softmax()\n",
        "\n",
        "inputs = np.array([[10, -10], [-10, 10], [10, -10]], dtype=float).T\n",
        "g = np.ones((2, 3))\n",
        "softmax(inputs)\n",
        "softmax.backwards1(g)\n",
        "grad1 = np.copy(softmax.input_grad)\n",
        "softmax(inputs)\n",
        "softmax.backwards2(g)\n",
        "grad2 = np.copy(softmax.input_grad)\n",
        "grad2 - grad1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHUrN5lpWy_n",
        "outputId": "294208a0-0af1-4493-85a6-7f2d8534b01b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.70447558e-17, -1.84756286e-25,  1.11022302e-16],\n",
              "       [-7.39775462e-17,  3.70447558e-17, -1.84756286e-25]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MNIST Dataset"
      ],
      "metadata": {
        "id": "NPH5bF-G6Act"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take the dataset from pytorch, convert to numpy\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define the transformation to apply to each image\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Download and load the MNIST dataset\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Convert the PyTorch datasets to NumPy arrays\n",
        "train_images = train_dataset.data.numpy()\n",
        "train_labels = train_dataset.targets.numpy()\n",
        "test_images = test_dataset.data.numpy()\n",
        "test_labels = test_dataset.targets.numpy()\n",
        "\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255\n",
        "\n",
        "# Print the shape of the arrays\n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "print(\"Train labels shape:\", train_labels.shape)\n",
        "print(\"Test images shape:\", test_images.shape)\n",
        "print(\"Test labels shape:\", test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XiMi5nod0wi",
        "outputId": "ce4237f4-18a0-46aa-cd07-ee19a6e66327"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 82335477.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 74867548.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 21159933.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3385556.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train images shape: (60000, 28, 28)\n",
            "Train labels shape: (60000,)\n",
            "Test images shape: (10000, 28, 28)\n",
            "Test labels shape: (10000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sequential(layers):\n",
        "  layers = layers[0:]\n",
        "  for i in range(0, len(layers) - 1):\n",
        "    layers[i].child = layers[i+1]\n",
        "  layers.reverse()\n",
        "  return layers\n",
        "\n",
        "class FCModel:\n",
        "  def __init__(self, input_shape=784):\n",
        "    self.softmax = Softmax()\n",
        "    self.loss = NLLLoss()\n",
        "    self.layers = [LinearLayer(input_shape, 512, bias=True),\n",
        "                   ReLU(),\n",
        "                   LinearLayer(512, 256, bias=True),\n",
        "                   ReLU(),\n",
        "                   LinearLayer(256, 10, bias=True),\n",
        "                   self.softmax]\n",
        "    self.layers[0].kaiming_init()\n",
        "    self.layers[2].kaiming_init()\n",
        "\n",
        "    self.softmax.child = self.loss\n",
        "    self.topsorted = [self.loss] + make_sequential(self.layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is batch_size x 28 x 28\n",
        "    x = x.reshape(x.shape[0], -1)\n",
        "    x = x.T\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "0bUzxCU7Yj_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(out, targets):\n",
        "  # targets are (BS,), out is (num_classes, BS) logits.\n",
        "  preds = np.argmax(out.T, axis=1)\n",
        "  # print(preds.shape, targets.shape)\n",
        "  correct = np.sum(preds == targets)\n",
        "  return correct, targets.shape[0]\n",
        "\n",
        "def test_get_accuracy(n=1000):\n",
        "  targets = np.ones((n,))\n",
        "  s = Softmax()\n",
        "  out = randn(10, n)\n",
        "  out = s(out)\n",
        "  return get_accuracy(out, targets)\n",
        "\n",
        "test_get_accuracy() # should be around 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egf_eazHazBG",
        "outputId": "8b16c0d0-4172-4559-b5a0-9828f7592b80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, train, valid, optimizer, args, label=\"\"):\n",
        "  # trains for one epoch\n",
        "  training_losses = []\n",
        "  valid_losses = []\n",
        "  valid_t, valid_c = 0, 0\n",
        "\n",
        "  # training\n",
        "  print(f\"[{label}]Training\")\n",
        "  for x, t in train.batch_iterator(args['batch_size']):\n",
        "    # forwards\n",
        "    x = model.forward(x)\n",
        "\n",
        "    # get loss and backpropagate\n",
        "    loss = model.loss(x, t)\n",
        "    training_losses.append(loss)\n",
        "    grads = back_propagate(model.topsorted)\n",
        "    optimizer.step(grads)\n",
        "\n",
        "  # validation\n",
        "  print(f\"[{label}]Evaluating\")\n",
        "  for x, t in valid.batch_iterator(args['batch_size']):\n",
        "    x = model.forward(x)\n",
        "    loss = model.loss(x, t)\n",
        "    valid_losses.append(loss)\n",
        "    correct, total = get_accuracy(x, t)\n",
        "    valid_t += total\n",
        "    valid_c += correct\n",
        "\n",
        "  tl = sum(training_losses) / len(training_losses)\n",
        "  vl = sum(valid_losses) / len(valid_losses)\n",
        "  acc = valid_c / valid_t\n",
        "  print(f\"Training loss {round(tl, 4)}, valid loss {round(vl, 4)}, valid acc {round(acc*100)}%\")\n",
        "  "
      ],
      "metadata": {
        "id": "qBxJwTjce9Db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = Dataset(train_images, train_labels)\n",
        "valid = Dataset(test_images, test_labels)\n",
        "print(f\"train is {train.dataset_size}, valid is {valid.dataset_size}\")\n",
        "\n",
        "optimizer = SGD(lr=1e-3)\n",
        "model = FCModel()\n",
        "args = {'batch_size': 64}\n",
        "\n",
        "for layer in model.layers:\n",
        "  print(layer)\n",
        "  if layer.child is None:\n",
        "    print(\"no child\")\n",
        "\n",
        "for i in range(10):\n",
        "  label = f\"Epoch {i+1}\"\n",
        "  train_one_epoch(model, train, valid, optimizer, args, label=label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIFoqlOYi_18",
        "outputId": "2525ad51-6f8d-4c20-a4a9-266b26287357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train is 60000, valid is 10000\n",
            "<__main__.LinearLayer object at 0x7f404002ca90>\n",
            "<__main__.ReLU object at 0x7f4040a7fd90>\n",
            "<__main__.LinearLayer object at 0x7f41108c3040>\n",
            "<__main__.ReLU object at 0x7f41108c1930>\n",
            "<__main__.LinearLayer object at 0x7f41108c3520>\n",
            "<__main__.Softmax object at 0x7f404002e860>\n",
            "[Epoch 1]Training\n",
            "[Epoch 1]Evaluating\n",
            "Training loss 160.617, valid loss 81.7236, valid acc 62%\n",
            "[Epoch 2]Training\n",
            "[Epoch 2]Evaluating\n",
            "Training loss 68.2646, valid loss 56.9234, valid acc 73%\n",
            "[Epoch 3]Training\n",
            "[Epoch 3]Evaluating\n",
            "Training loss 53.2787, valid loss 47.9233, valid acc 77%\n",
            "[Epoch 4]Training\n",
            "[Epoch 4]Evaluating\n",
            "Training loss 46.4706, valid loss 42.9524, valid acc 80%\n",
            "[Epoch 5]Training\n",
            "[Epoch 5]Evaluating\n",
            "Training loss 42.3122, valid loss 39.542, valid acc 81%\n",
            "[Epoch 6]Training\n",
            "[Epoch 6]Evaluating\n",
            "Training loss 39.4029, valid loss 37.1218, valid acc 82%\n",
            "[Epoch 7]Training\n",
            "[Epoch 7]Evaluating\n",
            "Training loss 37.1857, valid loss 35.2812, valid acc 83%\n",
            "[Epoch 8]Training\n",
            "[Epoch 8]Evaluating\n",
            "Training loss 35.4567, valid loss 33.7703, valid acc 84%\n",
            "[Epoch 9]Training\n",
            "[Epoch 9]Evaluating\n",
            "Training loss 34.0272, valid loss 32.5093, valid acc 84%\n",
            "[Epoch 10]Training\n",
            "[Epoch 10]Evaluating\n",
            "Training loss 32.8403, valid loss 31.5672, valid acc 85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kinda low accuracy, but the main takeaway is that this worked."
      ],
      "metadata": {
        "id": "mlxwttgGtpiY"
      }
    }
  ]
}