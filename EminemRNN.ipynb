{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EminemRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOBc+PHba7xQObXEQCPimpc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dungwoong/NN/blob/main/EminemRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DY-gUyuSol9B",
        "outputId": "e98f5364-6884-4ed7-ac96-1966c668e574"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 10.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 5.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# starter code from ageron/handson-ml2\n",
        "\n",
        "# Python ≥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Is this notebook running on Colab or Kaggle?\n",
        "IS_COLAB = \"google.colab\" in sys.modules\n",
        "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
        "\n",
        "if IS_COLAB:\n",
        "    %pip install -q -U tensorflow-addons\n",
        "    %pip install -q -U transformers\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
        "    if IS_KAGGLE:\n",
        "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"nlp\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eminem_url = \"https://raw.githubusercontent.com/dungwoong/NN/main/eminem/MASTER.txt\"\n",
        "filepath = keras.utils.get_file(\"eminem.txt\", eminem_url)\n",
        "with open(filepath) as f:\n",
        "  eminem_text = f.read()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c25e12fUonaK",
        "outputId": "52ffa14f-e1bf-4b9a-f4cd-58166ececd3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/dungwoong/NN/main/eminem/MASTER.txt\n",
            "106496/98894 [================================] - 0s 0us/step\n",
            "114688/98894 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to deal with \\u2005, \\u200b, \\u2060, 'á', 'ó', 'е'"
      ],
      "metadata": {
        "id": "pWCP6KBIv8_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_bad_chars = ['\\u2005', '\\u200b', '\\u2060', 'á', 'ó', 'е']\n",
        "for char in list_of_bad_chars:\n",
        "  print(char, \"---------------\")\n",
        "  idx = eminem_text.find(char)\n",
        "  print(eminem_text[idx-5:idx], \"HERE:\", eminem_text[idx:idx+5])\n",
        "  print(\"------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP1B0wHIvpZC",
        "outputId": "6a153d71-6a7b-49b0-a6ae-60c069ab8f85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ---------------\n",
            "g\n",
            "But HERE:  we'r\n",
            "------------\n",
            "​ ---------------\n",
            ".A.T. HERE: ​\n",
            "Her\n",
            "------------\n",
            "⁠ ---------------\n",
            "etter HERE: ⁠—*gu\n",
            "------------\n",
            "á ---------------\n",
            "up)\n",
            "C HERE: állat\n",
            "------------\n",
            "ó ---------------\n",
            ", adi HERE: ós\n",
            "I \n",
            "------------\n",
            "е ---------------\n",
            "ut ev HERE: еr si\n",
            "------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ok so I think first one is a space, second is a \\n, third one is a space...?\n",
        "eminem_text = eminem_text.replace('\\u2005', ' ')\n",
        "eminem_text = eminem_text.replace('\\u200b', '\\n')\n",
        "eminem_text = eminem_text.replace('\\u2060', ' ')\n",
        "eminem_text = eminem_text.replace('á', 'a')\n",
        "eminem_text = eminem_text.replace('ó', 'o')\n",
        "eminem_text = eminem_text.replace('е', 'e')\n",
        "eminem_text = eminem_text.replace('’', \"'\")"
      ],
      "metadata": {
        "id": "qhitTYv2xPZA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "X6FgS389pI2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(eminem_text)"
      ],
      "metadata": {
        "id": "pibIkmnbpBzR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences([\"First\", \"Second\"]))\n",
        "print(tokenizer.sequences_to_texts([[25, 4, 10, 9, 3], [9, 2, 17, 6, 7, 14]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TE4WfTo8pGIc",
        "outputId": "52fa9714-d683-41f4-f764-ace688d59af6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[25, 4, 10, 9, 3], [9, 2, 17, 6, 7, 14]]\n",
            "['f i r s t', 's e c o n d']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count # total number of characters"
      ],
      "metadata": {
        "id": "kEmpuPFDpSyj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_id, dataset_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GauTQbjmpVhZ",
        "outputId": "30949523-9395-46c0-f739-ff8bd6d28838"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57 96447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "complete_text = tokenizer.sequences_to_texts([np.arange(1, max_id, step=1)])\n",
        "print(sorted(complete_text[0].split(\" \")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CDYVR2oyINr",
        "outputId": "d306da1e-d424-458d-c960-d7f51bef63ac"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', '\\n', '!', '\"', '$', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '—']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "shakespeare dataset size was 1115394 so it was legit 100 times larger"
      ],
      "metadata": {
        "id": "PjFnUuxepYg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# note that u put in list, cuz there's a list for documents, but we'll only have 1 document\n",
        "[encoded] = np.array(tokenizer.texts_to_sequences([eminem_text])) - 1"
      ],
      "metadata": {
        "id": "l6K_3tKapdfp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful RNNs\n",
        "\n",
        "I forgot my shakespeare model was tokenized differently than the eminem one so I can't do transfer learning :((((\n",
        "\n",
        "This is how you'd do it tho\n",
        "\n",
        "```weights = old_model_layer.get_weights()```\n",
        "\n",
        "```new_model_layer.set_weights(weights)```"
      ],
      "metadata": {
        "id": "RsCSqZRYpvBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "batch_size = 1\n",
        "n_steps = 100\n",
        "window_length = n_steps + 1"
      ],
      "metadata": {
        "id": "jZZBhB6cp_OY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
        "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
        "dataset = dataset.batch(1)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
        "dataset = dataset.map(\n",
        "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset = dataset.prefetch(1)"
      ],
      "metadata": {
        "id": "pZfh_zempoCF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you put recurrent dropout you can't use GPU\n",
        "model = keras.models.Sequential([\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, batch_input_shape=[batch_size, None, max_id]),\n",
        "                                 keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2),\n",
        "                                 keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
        "])\n",
        "\n",
        "class ResetStatesCallback(keras.callbacks.Callback):\n",
        "  def on_epoch_begin(self, epoch, logs):\n",
        "    self.model.reset_states()\n",
        "\n",
        "cb_checkpoint = keras.callbacks.ModelCheckpoint('model.h5')"
      ],
      "metadata": {
        "id": "iT4jm-NmrGA5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
        "history = model.fit(dataset, epochs=50, callbacks=[ResetStatesCallback(), cb_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "claWyl2VtqnP",
        "outputId": "e88c8fde-a451-4eda-d95e-7f6cba72b228"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "868/868 [==============================] - 19s 13ms/step - loss: 2.6474\n",
            "Epoch 2/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.3319\n",
            "Epoch 3/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.2172\n",
            "Epoch 4/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.1522\n",
            "Epoch 5/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.1050\n",
            "Epoch 6/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.0678\n",
            "Epoch 7/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.0336\n",
            "Epoch 8/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 2.0125\n",
            "Epoch 9/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9963\n",
            "Epoch 10/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9691\n",
            "Epoch 11/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9515\n",
            "Epoch 12/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9380\n",
            "Epoch 13/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9238\n",
            "Epoch 14/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.9146\n",
            "Epoch 15/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.9031\n",
            "Epoch 16/50\n",
            "868/868 [==============================] - 12s 13ms/step - loss: 1.8929\n",
            "Epoch 17/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8870\n",
            "Epoch 18/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8699\n",
            "Epoch 19/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8726\n",
            "Epoch 20/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8575\n",
            "Epoch 21/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8549\n",
            "Epoch 22/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8496\n",
            "Epoch 23/50\n",
            "868/868 [==============================] - 12s 13ms/step - loss: 1.8360\n",
            "Epoch 24/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8409\n",
            "Epoch 25/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8291\n",
            "Epoch 26/50\n",
            "868/868 [==============================] - 13s 15ms/step - loss: 1.8251\n",
            "Epoch 27/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8160\n",
            "Epoch 28/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8103\n",
            "Epoch 29/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.8078\n",
            "Epoch 30/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.8007\n",
            "Epoch 31/50\n",
            "868/868 [==============================] - 12s 13ms/step - loss: 1.7964\n",
            "Epoch 32/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7865\n",
            "Epoch 33/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7961\n",
            "Epoch 34/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7852\n",
            "Epoch 35/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7776\n",
            "Epoch 36/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7705\n",
            "Epoch 37/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7737\n",
            "Epoch 38/50\n",
            "868/868 [==============================] - 12s 13ms/step - loss: 1.7704\n",
            "Epoch 39/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7629\n",
            "Epoch 40/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7608\n",
            "Epoch 41/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7616\n",
            "Epoch 42/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7481\n",
            "Epoch 43/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7530\n",
            "Epoch 44/50\n",
            "868/868 [==============================] - 12s 13ms/step - loss: 1.7502\n",
            "Epoch 45/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7468\n",
            "Epoch 46/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7414\n",
            "Epoch 47/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7386\n",
            "Epoch 48/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7412\n",
            "Epoch 49/50\n",
            "868/868 [==============================] - 12s 14ms/step - loss: 1.7379\n",
            "Epoch 50/50\n",
            "868/868 [==============================] - 11s 13ms/step - loss: 1.7367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "metadata": {
        "id": "lguKIUjeuO3l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = preprocess([\"How are yo\"])\n",
        "#Y_pred = model.predict_classes(X_new)\n",
        "Y_pred = np.argmax(model(X_new), axis=-1)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "I5YW1XyMubpr",
        "outputId": "9ef123ae-e71f-494e-90d5-17e8f8a596ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WTF IS DIS BRAH"
      ],
      "metadata": {
        "id": "8dtaSeIVueO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def next_char(text, temperature=1):\n",
        "    X_new = preprocess([text])\n",
        "    y_proba = model(X_new)[0, -1:, :] # last set of probabilities\n",
        "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
        "\n",
        "# generates text I think\n",
        "def complete_text(text, n_chars=50, temperature=1):\n",
        "    for _ in range(n_chars):\n",
        "        text += next_char(text, temperature)\n",
        "    return text"
      ],
      "metadata": {
        "id": "WX9y_DokufQw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for temperature in [0.2, 0.4, 0.6, 0.8, 1, 2]:\n",
        "  print(\"TEMPERATURE:\", temperature, \"--------------\")\n",
        "  print(complete_text(\"t\", n_chars=400, temperature=temperature))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NK6mGEM1Rklv",
        "outputId": "78bd0407-0285-4181-cccc-de4bcfa6734d"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEMPERATURE: 0.2 --------------\n",
            "t it in the motherfuckin' been (woo!)\n",
            "that's how much we have in common (woo!)\n",
            "but i got a stop and i made in common (yah!)\n",
            "that's how much we have in common (woo!)\n",
            "i said, \"i don't know is everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on ever\n",
            "TEMPERATURE: 0.4 --------------\n",
            "ty in one (yeah)\n",
            "i'm the tayin' the fuck to the fuck out with no stop (yeah)\n",
            "\n",
            "[chorus]\n",
            "the coupless i got a fuckin' deck to stuck into everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, fack, fack on everyon\n",
            "TEMPERATURE: 0.6 --------------\n",
            "ter, i had a suckin' ind it on the record black in the moct the scause i was nothin' through how the mole tire heart and i'm totermed in the starter\n",
            "i don't can a hotter crayer, can't even kninksed a pullers\n",
            "and i'm the killer with the dogin' with me the cops and i know when i'm fromt\n",
            "i've been colin' into everyone\n",
            "fack, fings into everyone\n",
            "fack, fack, fack on everyone\n",
            "fack, fack on everyone\n",
            "fack, \n",
            "TEMPERATURE: 0.8 --------------\n",
            "tfay, ma-ha\n",
            "jacts, i mon't saye (sounder)\n",
            "and i'm gonna hot to go in my dryged\n",
            "so shes rove arould in it (oh)\n",
            "chack and every times the way\n",
            "i ain't we ding and alpows with deredicl n've even donnt like a blowers beet her in common (woo!)\n",
            "inside a froat rocks i'm talkin' 'bout to be read your cups six (yeah) i'm a kamikaze\n",
            "ambettin', it just should be like neeten\n",
            "evenything is down the danger when a\n",
            "TEMPERATURE: 1 --------------\n",
            "thy'slle)\n",
            "i'm the beat is at me, andrit these calk once clusk on sound af\n",
            "even doig' hid up and oli monna fuckin' finger\n",
            "you suckin' recive on cockin' the cantle turn, i'm a twaded like the money yet the snith onticlele, ttan it's contint? (yeah) you get through the flith\n",
            "bet i've just grating in the motherfuckin' dick my cra\n",
            "\n",
            "you guts heads cimmoisso the mentaclet! (hah!)\n",
            "that's how much se have i\n",
            "TEMPERATURE: 2 --------------\n",
            "thin' hob y log'siirn\n",
            "so sreggex, acelumipobpr\n",
            "aye pab-unce?]\n",
            "wherc,mshit \"toink\n",
            "gitdomemamxnul\n",
            "kally, y'alo whocq\n",
            "drfelllcosinea\n",
            "unrsamy-coak alvultl'?llece juevoat leyer,\n",
            "b'bliomvevely sizyge? and beaht yejw, cops 'bout do, toolevowh,ry'all bwn-ecbpusid)\n",
            "taynis irowneps atdaxecen clea\n",
            "\n",
            "[vhna-11]mop anygoodawin', pams hizbin' you peyco?\n",
            "sprold as, gun' huck wourd yot,tsime \n",
            "th you  ug\n",
            "des,rg junbl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fack on everyone is the chorus of Kamikaze. Definitely need better processing. However, the input data is really small, and it makes sense that choruses are showing up(because they have such a high probability of occurring compared to everything else)\n",
        "\n",
        "If I set temperature too high then I get random stuff and it's not even words."
      ],
      "metadata": {
        "id": "-NPnTrqrT7EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", n_chars=800, temperature=0.6))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl1TQDChRoPq",
        "outputId": "07767830-607f-4ea0-a38b-1eccafad699b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "traver i have in common (woo!)\n",
            "i'm a fuckin' bijaes (yeah)\n",
            "\n",
            "[chorus]\n",
            "now your anders, but i'm gone wond to spit the back of the corner\n",
            "treats in a strayg when i was a better fuckin' dictic\n",
            "but in the windows the some hit\n",
            "to greaters to me and down\n",
            "cheat to go f-oe and i been gonit to cymborsic, i'm fuckin' weing\n",
            "i have no stuck of anyrody cangs and i ain't got your i find record\n",
            "but i'm a tige cugs out and some moon and i've not to be coles\n",
            "but like i'm the more and the treater back to mure to the world you only the trippin' like the coater\n",
            "and i'm a side a motherfuckin' been in a digher called and heart and the controin (woo!)\n",
            "that's how much we have in common (what?)\n",
            "i'm not snap (i'm through ind the controm me me way the motherfuckers\n",
            "in a couple of the letter une and i say, \"damn, chask\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", n_chars=1200, temperature=0.7))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ctJa2-_R20k",
        "outputId": "90512888-667d-483c-f543-b03150ef0032"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the word (yeah!)\n",
            "no some money out it a punk (fack)\n",
            "i'm on a right are at reap you\n",
            "shreaked the kimikaze, gonna\n",
            "shoulda might and a vock lyating to pen site right's endin' is one\n",
            "the wellin' a sale on the care is up like a pillar rips\n",
            "i can't gotta tool and you with a windred here in common (i'm a domp)\n",
            "may, then i got a fuckin' dezike into everyone\n",
            "fack, fack on everyone\n",
            "fack, fidd 'cause i got a scourply for me one bagk\n",
            "at might and it's colia then i'm a (yeah!)\n",
            "that's how much we have in common (woo!)\n",
            "now you wrote it and i'm s uncompon'ly these call you slawly\n",
            "i hell the bules like a concome, real every time i'm gonna\n",
            "god me turn in common (yeah)\n",
            "i'm better be go night the sleefin'\n",
            "like a lough the decks abrun chome\n",
            "i don't even got a greated 'em undered up on my norgligais\n",
            "so you frockin' like i rush me\n",
            "want i wanna time up on the want rimb bulated up like a brog, hope in my tool, i know as explobumb on the plan (what?)\n",
            "i been gonna botton a punk and she's a becks\n",
            "i can sand in my next to me in a sunoled in cock in aurceles\n",
            "in ackec to the motherfuckin' shit\n",
            "i do not scare came cops citched in the mic (yeah!)\n",
            "i don't gotta reven enerdacalday duckin' deady\n",
            "i don't ever lofe neve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", n_chars=300, temperature=0.5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKnhKbtxUXlm",
        "outputId": "73f1286f-a219-4c1d-af7d-ad9cf2218b9a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ter in the crade in common (woo!!)\n",
            "but i don't kame and i told me for the moner\n",
            "i'm a killer but if you can't get for me on your hundred in the recorts\n",
            "i heard the one the conce from her scints a could straught off of me with me rope in the becked in the wand to put the track in the dect\n",
            "i'm fuckin' \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(complete_text(\"t\", n_chars=300, temperature=0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBtOJLv-WtBE",
        "outputId": "35510461-3487-4302-84bc-fdb5f9612d61"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tracked in the motherfuckin' record be the controls\n",
            "i don't take a tropped in the controls and i say i say i'm a (what?)\n",
            "i don't know what i stick in the mic (yeah)\n",
            "that's how much we have in common (woa!)\n",
            "i feel like a fuckin' dick and i'm a kamikaze, kamikaze, kamikaze (kamikaze, kamikaze, kamikaze\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "evj0NRsZXF8T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}